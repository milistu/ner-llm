{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import (\n",
    "    ClassLabel,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"~/Development/entity-recognition-datasets\").expanduser()\n",
    "assert repo_path.exists(), \"Please clone the repository with the datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(words: List[str], ner_tags: List[int]) -> str:\n",
    "\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        full_label = tag\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "    return line1 + \"\\n\" + line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_data(\n",
    "    file_path: Path, split_by: str = \"\\t\"\n",
    ") -> Dict[int, Dict[str, List[str]]]:\n",
    "    dataset = []\n",
    "    sentence_id = 0\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates a sentence boundary\n",
    "                if current_words:  # If we have collected words and tags for a sentence\n",
    "                    dataset.append(\n",
    "                        {\n",
    "                            \"id\": str(sentence_id),\n",
    "                            \"words\": current_words,\n",
    "                            \"ner_tags\": current_tags,\n",
    "                        }\n",
    "                    )\n",
    "                    sentence_id += 1\n",
    "                    current_words = []\n",
    "                    current_tags = []\n",
    "            else:\n",
    "                word, tag = line.split(split_by)  # Split by tab\n",
    "                current_words.append(word)\n",
    "                current_tags.append(tag)\n",
    "\n",
    "        # Append the last sentence if the file doesn't end with a newline\n",
    "        if current_words:\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"id\": str(sentence_id),\n",
    "                    \"words\": current_words,\n",
    "                    \"ner_tags\": current_tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(labels: List[str]) -> List[str]:\n",
    "    \"\"\"This method fixes the labels to be in the correct IOB format. Wikigold only has O and I tags, but we need to have B, I and O tags.\"\"\"\n",
    "    if any([\"-\" in label for label in labels if label != \"O\"]):\n",
    "        labels = [label.split(\"-\")[-1] for label in labels]\n",
    "\n",
    "    new_labels = []\n",
    "    current_label = None\n",
    "    for label in labels:\n",
    "        if current_label is None:\n",
    "            current_label = label\n",
    "            if label == \"O\":\n",
    "                new_labels.append(label)\n",
    "            else:\n",
    "                new_labels.append(f\"B-{label}\")\n",
    "        else:\n",
    "            if label == \"O\":\n",
    "                new_labels.append(label)\n",
    "                current_label = label\n",
    "            else:\n",
    "                if label == current_label:\n",
    "                    new_labels.append(f\"I-{current_label}\")\n",
    "                else:\n",
    "                    new_labels.append(f\"B-{label}\")\n",
    "                    current_label = label\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def pust_to_hf(\n",
    "    repo_id: str,\n",
    "    label_names: List[str],\n",
    "    data_files: Dict[str, str] | str,\n",
    "    test_split_percentage: float = None,\n",
    ") -> DatasetDict:\n",
    "    features = Features(\n",
    "        {\n",
    "            \"id\": Value(dtype=\"string\"),\n",
    "            \"words\": Sequence(feature=Value(dtype=\"string\")),\n",
    "            \"ner_tags\": Sequence(feature=ClassLabel(names=label_names)),\n",
    "        }\n",
    "    )\n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    if test_split_percentage:\n",
    "        dataset = dataset[\"train\"].train_test_split(\n",
    "            test_size=test_split_percentage, seed=42\n",
    "        )\n",
    "\n",
    "    dataset.push_to_hub(repo_id=repo_id)\n",
    "    print(f\"Dataset {repo_id} pushed to the hub\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def sort_labels(labels):\n",
    "    # Separate the 'O' label from the rest\n",
    "    o_labels = [label for label in labels if label == \"O\"]\n",
    "\n",
    "    # Separate B- labels and corresponding I- labels\n",
    "    b_labels = sorted([label for label in labels if label.startswith(\"B-\")])\n",
    "    i_labels = [label for label in labels if label.startswith(\"I-\")]\n",
    "\n",
    "    # Sort I- labels based on their corresponding B- labels\n",
    "    sorted_labels = o_labels  # 'O' first\n",
    "    for b_label in b_labels:\n",
    "        sorted_labels.append(b_label)\n",
    "        # Add the corresponding I- label\n",
    "        corresponding_i_labels = [\n",
    "            i_label for i_label in i_labels if i_label[2:] == b_label[2:]\n",
    "        ]\n",
    "        sorted_labels.extend(corresponding_i_labels)\n",
    "\n",
    "    return sorted_labels\n",
    "\n",
    "\n",
    "def print_analysis(df: pd.DataFrame, format: Literal[\"count\", \"percentage\"] = \"count\"):\n",
    "    print(f\"Number of sentences in the dataset: {len(df)}\")\n",
    "    if format == \"count\":\n",
    "        print(\n",
    "            f\"Label count in dataset:\\n{df['ner_tags'].explode().value_counts().to_markdown()}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Label percentage in dataset:\\n{(df['ner_tags'].explode().value_counts(normalize=True) * 100).to_markdown()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def vizualize_ner_dataset(dataset: DatasetDict, num_samples: int = 5):\n",
    "    if num_samples:\n",
    "        random_ids = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "        print(\"=\" * 50 + \"START\" + \"=\" * 50)\n",
    "        for id in random_ids:\n",
    "            print(f\"Sentence ID: {id}\")\n",
    "            print(check_labels(dataset[id][\"words\"], dataset[id][\"ner_tags\"]))\n",
    "            print(\"-\" * 100)\n",
    "        print(\"=\" * 50 + \"END\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "def get_statistics(dataset: DatasetDict):\n",
    "    for split in dataset:\n",
    "        print(f\"Split: {split}\")\n",
    "        df = dataset[split].to_pandas()\n",
    "        print_analysis(df, format=\"count\")\n",
    "        print_analysis(df, format=\"percentage\")\n",
    "\n",
    "\n",
    "def proces_ner_dataset(\n",
    "    file_path: Path,\n",
    "    output_path: Path = None,\n",
    "    split_by: str = \"\\t\",\n",
    "    vizualize: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    assert file_path.exists(), f\"File not found at {file_path}\"\n",
    "    if output_path:\n",
    "        assert output_path.suffix == \".jsonl\", \"Output path should be a JSONL file\"\n",
    "        output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    raw_data = load_conll_data(file_path, split_by=split_by)\n",
    "    print(f\"Number of sentences in the train set: {len(raw_data)}\")\n",
    "\n",
    "    vizualize_ner_dataset(raw_data, num_samples=vizualize)\n",
    "\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    if output_path is not None:\n",
    "        df.to_json(\n",
    "            path_or_buf=output_path,\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the train set: 1841\n"
     ]
    }
   ],
   "source": [
    "file_path = repo_path / \"data/wikigold/CONLL-format/data/wikigold.conll.txt\"\n",
    "dataset_name = file_path.parts[6]\n",
    "output_path = output_dir / dataset_name / \"wikigold.jsonl\"\n",
    "\n",
    "df = proces_ner_dataset(file_path, split_by=\" \", vizualize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ner_tags\"] = df[\"ner_tags\"].apply(format_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df[\"ner_tags\"].explode().value_counts().keys())\n",
    "label_names = sort_labels(label_names)\n",
    "\n",
    "dataset = pust_to_hf(\n",
    "    repo_id=\"Studeni/Wikigold-NER-conll\",\n",
    "    label_names=label_names,\n",
    "    data_files=str(output_path),\n",
    "    test_split_percentage=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_statistics(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = repo_path / \"data/GUM/CONLL-format/data/train/gum-train.conll\"\n",
    "assert file_path_train.exists(), f\"File not found at {file_path_train}\"\n",
    "file_path_test = repo_path / \"data/GUM/CONLL-format/data/test/gum-test.conll\"\n",
    "assert file_path_test.exists(), f\"File not found at {file_path_test}\"\n",
    "\n",
    "dataset_name = \"GUM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_train = output_dir / dataset_name / \"gum-train.jsonl\"\n",
    "\n",
    "df_train = proces_ner_dataset(file_path_train, output_path, split_by=\"\\t\", vizualize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_test = output_dir / dataset_name / \"gum-test.jsonl\"\n",
    "\n",
    "df_test = proces_ner_dataset(file_path_test, split_by=\"\\t\", vizualize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df_train[\"ner_tags\"].explode().value_counts().keys())\n",
    "label_names_sorted = sort_labels(label_names)\n",
    "\n",
    "dataset = pust_to_hf(\n",
    "    repo_id=\"Studeni/GUM-NER-conll\",\n",
    "    label_names=label_names_sorted,\n",
    "    data_files={\"train\": str(output_path_train), \"test\": str(output_path_test)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
