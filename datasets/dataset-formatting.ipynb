{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import (\n",
    "    ClassLabel,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"~/Development/entity-recognition-datasets\").expanduser()\n",
    "assert repo_path.exists(), \"Please clone the repository with the datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(words: List[str], ner_tags: List[int]) -> str:\n",
    "\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        full_label = tag\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "    return line1 + \"\\n\" + line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_data(\n",
    "    file_path: Path, split_by: str = \"\\t\"\n",
    ") -> Dict[int, Dict[str, List[str]]]:\n",
    "    dataset = []\n",
    "    sentence_id = 0\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates a sentence boundary\n",
    "                if current_words:  # If we have collected words and tags for a sentence\n",
    "                    dataset.append(\n",
    "                        {\n",
    "                            \"id\": str(sentence_id),\n",
    "                            \"words\": current_words,\n",
    "                            \"ner_tags\": current_tags,\n",
    "                        }\n",
    "                    )\n",
    "                    sentence_id += 1\n",
    "                    current_words = []\n",
    "                    current_tags = []\n",
    "            else:\n",
    "                word, tag = line.split(split_by)  # Split by tab\n",
    "                current_words.append(word)\n",
    "                current_tags.append(tag)\n",
    "\n",
    "        # Append the last sentence if the file doesn't end with a newline\n",
    "        if current_words:\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"id\": str(sentence_id),\n",
    "                    \"words\": current_words,\n",
    "                    \"ner_tags\": current_tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(labels: List[str]) -> List[str]:\n",
    "    \"\"\"This method fixes the labels to be in the correct IOB format. Wikigold only has O and I tags, but we need to have B, I and O tags.\"\"\"\n",
    "    if any([\"-\" in label for label in labels if label != \"O\"]):\n",
    "        labels = [label.split(\"-\")[-1] for label in labels]\n",
    "\n",
    "    new_labels = []\n",
    "    current_label = None\n",
    "    for label in labels:\n",
    "        if current_label is None:\n",
    "            current_label = label\n",
    "            if label == \"O\":\n",
    "                new_labels.append(label)\n",
    "            else:\n",
    "                new_labels.append(f\"B-{label}\")\n",
    "        else:\n",
    "            if label == \"O\":\n",
    "                new_labels.append(label)\n",
    "                current_label = label\n",
    "            else:\n",
    "                if label == current_label:\n",
    "                    new_labels.append(f\"I-{current_label}\")\n",
    "                else:\n",
    "                    new_labels.append(f\"B-{label}\")\n",
    "                    current_label = label\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def pust_to_hf(\n",
    "    repo_id: str,\n",
    "    label_names: List[str],\n",
    "    data_files: Dict[str, str] | str,\n",
    "    test_split_percentage: float = None,\n",
    ") -> DatasetDict:\n",
    "    features = Features(\n",
    "        {\n",
    "            \"id\": Value(dtype=\"string\"),\n",
    "            \"words\": Sequence(feature=Value(dtype=\"string\")),\n",
    "            \"ner_tags\": Sequence(feature=ClassLabel(names=label_names)),\n",
    "        }\n",
    "    )\n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    if test_split_percentage:\n",
    "        dataset = dataset[\"train\"].train_test_split(\n",
    "            test_size=test_split_percentage, seed=42\n",
    "        )\n",
    "\n",
    "    dataset.push_to_hub(repo_id=repo_id)\n",
    "    print(f\"Dataset {repo_id} pushed to the hub\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def sort_labels(labels):\n",
    "    # Separate the 'O' label from the rest\n",
    "    o_labels = [label for label in labels if label == \"O\"]\n",
    "\n",
    "    # Separate B- labels and corresponding I- labels\n",
    "    b_labels = sorted([label for label in labels if label.startswith(\"B-\")])\n",
    "    i_labels = [label for label in labels if label.startswith(\"I-\")]\n",
    "\n",
    "    # Sort I- labels based on their corresponding B- labels\n",
    "    sorted_labels = o_labels  # 'O' first\n",
    "    for b_label in b_labels:\n",
    "        sorted_labels.append(b_label)\n",
    "        # Add the corresponding I- label\n",
    "        corresponding_i_labels = [\n",
    "            i_label for i_label in i_labels if i_label[2:] == b_label[2:]\n",
    "        ]\n",
    "        sorted_labels.extend(corresponding_i_labels)\n",
    "\n",
    "    return sorted_labels\n",
    "\n",
    "\n",
    "def print_analysis(df: pd.DataFrame, format: Literal[\"count\", \"percentage\"] = \"count\"):\n",
    "    print(f\"Number of sentences in the dataset: {len(df)}\")\n",
    "    if format == \"count\":\n",
    "        print(\n",
    "            f\"Label count in dataset:\\n{df['ner_tags'].explode().value_counts().to_markdown()}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Label percentage in dataset:\\n{(df['ner_tags'].explode().value_counts(normalize=True) * 100).to_markdown()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def vizualize_ner_dataset(dataset: DatasetDict, num_samples: int = 5):\n",
    "    if num_samples:\n",
    "        random_ids = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "        print(\"=\" * 50 + \"START\" + \"=\" * 50)\n",
    "        for id in random_ids:\n",
    "            print(f\"Sentence ID: {id}\")\n",
    "            print(check_labels(dataset[id][\"words\"], dataset[id][\"ner_tags\"]))\n",
    "            print(\"-\" * 100)\n",
    "        print(\"=\" * 50 + \"END\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "def get_statistics(dataset: DatasetDict):\n",
    "    for split in dataset:\n",
    "        print(f\"Split: {split}\")\n",
    "        df = dataset[split].to_pandas()\n",
    "        print_analysis(df, format=\"count\")\n",
    "        print_analysis(df, format=\"percentage\")\n",
    "\n",
    "\n",
    "def proces_ner_dataset(\n",
    "    file_path: Path,\n",
    "    output_path: Path = None,\n",
    "    split_by: str = \"\\t\",\n",
    "    vizualize: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    assert file_path.exists(), f\"File not found at {file_path}\"\n",
    "    if output_path:\n",
    "        assert output_path.suffix == \".jsonl\", \"Output path should be a JSONL file\"\n",
    "        output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    raw_data = load_conll_data(file_path, split_by=split_by)\n",
    "    print(f\"Number of sentences in the train set: {len(raw_data)}\")\n",
    "\n",
    "    vizualize_ner_dataset(raw_data, num_samples=vizualize)\n",
    "\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    if output_path is not None:\n",
    "        df.to_json(\n",
    "            path_or_buf=output_path,\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the train set: 1841\n"
     ]
    }
   ],
   "source": [
    "file_path = repo_path / \"data/wikigold/CONLL-format/data/wikigold.conll.txt\"\n",
    "dataset_name = file_path.parts[6]\n",
    "output_path = output_dir / dataset_name / \"wikigold.jsonl\"\n",
    "\n",
    "df = proces_ner_dataset(file_path, split_by=\" \", vizualize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ner_tags\"] = df[\"ner_tags\"].apply(format_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df[\"ner_tags\"].explode().value_counts().keys())\n",
    "label_names = sort_labels(label_names)\n",
    "\n",
    "dataset = pust_to_hf(\n",
    "    repo_id=\"Studeni/Wikigold-NER-conll\",\n",
    "    label_names=label_names,\n",
    "    data_files=str(output_path),\n",
    "    test_split_percentage=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_statistics(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = repo_path / \"data/GUM/CONLL-format/data/train/gum-train.conll\"\n",
    "assert file_path_train.exists(), f\"File not found at {file_path_train}\"\n",
    "file_path_test = repo_path / \"data/GUM/CONLL-format/data/test/gum-test.conll\"\n",
    "assert file_path_test.exists(), f\"File not found at {file_path_test}\"\n",
    "\n",
    "dataset_name = \"GUM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_train = output_dir / dataset_name / \"gum-train.jsonl\"\n",
    "\n",
    "df_train = proces_ner_dataset(file_path_train, output_path, split_by=\"\\t\", vizualize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_test = output_dir / dataset_name / \"gum-test.jsonl\"\n",
    "\n",
    "df_test = proces_ner_dataset(file_path_test, split_by=\"\\t\", vizualize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df_train[\"ner_tags\"].explode().value_counts().keys())\n",
    "label_names_sorted = sort_labels(label_names)\n",
    "\n",
    "dataset = pust_to_hf(\n",
    "    repo_id=\"Studeni/GUM-NER-conll\",\n",
    "    label_names=label_names_sorted,\n",
    "    data_files={\"train\": str(output_path_train), \"test\": str(output_path_test)},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = Path(\"../cache\")\n",
    "assert cache_dir.exists(), f\"Cache directory not found at {cache_dir}\"\n",
    "dataset = load_dataset(\n",
    "    \"Studeni/Pile-NER-type-conll\", cache_dir=cache_dir, split=\"train\"\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"labels_raw\"] = dataset[\"labels\"].apply(\n",
    "    lambda labels: set([label.split(\"-\")[-1] for label in labels if label != \"O\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12654"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"labels_raw\"].explode().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_DESCRIPTION_PROMPT = \"\"\"\n",
    "###TASK###\n",
    "Your task is to generate three different variety descriptions for the target NER (Named Entity Recognition) label based on the provided word pairs and NER labels. \n",
    "Each description should be one sentence long and may include examples if needed to better explain the label. \n",
    "The examples must be generalist and not specific to any particular domain.\n",
    "\n",
    "###INSTRUCTIONS###\n",
    "Follow these steps:\n",
    "\n",
    "1. Analyze the word pairs:\n",
    "   - Look for patterns and commonalities among the words in each pair.\n",
    "   - Consider how these words relate to the target NER label.\n",
    "\n",
    "2. Generate descriptions:\n",
    "   - Create three distinct, one-sentence descriptions for the target label.\n",
    "   - Sentences needs to be direct and concise.\n",
    "   - Each description should capture the essence of the label based on the patterns observed in the word pairs.\n",
    "   - If necessary, include generalist examples to clarify the label's meaning.\n",
    "   - Ensure that the descriptions are varied in their approach and wording.\n",
    "\n",
    "3. Format the output:\n",
    "   - Present your generated descriptions in JSON format.\n",
    "   - Use the keys \"description_1\", \"description_2\", and \"description_3\" for each description.\n",
    "\n",
    "###OUTPUT EXAMPLE###\n",
    "Your final output should be structured as follows:\n",
    "\n",
    "{{\n",
    "   \"description_1\": \"Your first generated description\",\n",
    "   \"description_2\": \"Your second generated description\",\n",
    "   \"description_3\": \"Your third generated description\"\n",
    "}}\n",
    "\n",
    "###SUMMARY###\n",
    "Remember to make your descriptions clear, concise, and informative, focusing on the general concept of the NER label rather than specific instances.\n",
    "\n",
    "###INPUT###\n",
    "List of words:\n",
    "{words}\n",
    "\n",
    "List of NER labels:\n",
    "{ner_labels}\n",
    "\n",
    "Target label:\n",
    "{target_label}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    description_1: str\n",
    "    description_2: str\n",
    "    description_3: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptions(\n",
    "    client: OpenAI,\n",
    "    prompt: str,\n",
    "    response_format: BaseModel,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format=response_format,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    45889.000000\n",
       "mean         7.701911\n",
       "std          4.621669\n",
       "min          0.000000\n",
       "25%          4.000000\n",
       "50%          7.000000\n",
       "75%         10.000000\n",
       "max         44.000000\n",
       "Name: labels_raw, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"labels_raw\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = LABELS_DESCRIPTION_PROMPT.format(\n",
    "    words=dataset.iloc[0].words,\n",
    "    ner_labels=dataset.iloc[0].ner_tags,\n",
    "    target_label=\"PROGRAMMING_CONCEPT\",\n",
    ")\n",
    "\n",
    "response = get_descriptions(client, prompt, Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description_1': 'A programming concept refers to an abstract idea or principle that is fundamental to the practice of programming, such as variables, loops, and functions, which help in structuring and organizing code.',\n",
       " 'description_2': 'Programming concepts encompass the foundational elements of coding, including data types, control structures, and algorithms, which are essential for developing software applications.',\n",
       " 'description_3': 'Examples of programming concepts include object-oriented programming, which focuses on the use of objects and classes, and functional programming, which emphasizes the use of functions as the primary building blocks of software.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedChatCompletion[Response](id='chatcmpl-ADdpP41ccK38FOYBBZ1kVe3C7kQzh', choices=[ParsedChoice[Response](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[Response](content='{\\n   \"description_1\": \"A programming concept refers to an abstract idea or principle that is fundamental to the practice of programming, such as variables, loops, and functions, which help in structuring and organizing code.\",\\n   \"description_2\": \"Programming concepts encompass the foundational elements of coding, including data types, control structures, and algorithms, which are essential for developing software applications.\",\\n   \"description_3\": \"Examples of programming concepts include object-oriented programming, which focuses on the use of objects and classes, and functional programming, which emphasizes the use of functions as the primary building blocks of software.\"\\n}', refusal=None, role='assistant', function_call=None, tool_calls=[], parsed=Response(description_1='A programming concept refers to an abstract idea or principle that is fundamental to the practice of programming, such as variables, loops, and functions, which help in structuring and organizing code.', description_2='Programming concepts encompass the foundational elements of coding, including data types, control structures, and algorithms, which are essential for developing software applications.', description_3='Examples of programming concepts include object-oriented programming, which focuses on the use of objects and classes, and functional programming, which emphasizes the use of functions as the primary building blocks of software.')))], created=1727814727, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f85bea6784', usage=CompletionUsage(completion_tokens=124, prompt_tokens=2054, total_tokens=2178, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description_1': 'A programming concept refers to a fundamental idea or principle that underlies the structure and behavior of programming languages, such as variables, loops, and functions.',\n",
       " 'description_2': 'Programming concepts are the building blocks of software development, encompassing techniques and methodologies like object-oriented programming and recursion that guide how code is written and organized.',\n",
       " 'description_3': 'Examples of programming concepts include data types, control structures, and algorithms, which are essential for creating efficient and effective software solutions.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedChatCompletion[Response](id='chatcmpl-ADe0DwY94T92uQihlkbUe9ZrMpJ12', choices=[ParsedChoice[Response](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[Response](content='{\"description_1\":\"A programming concept refers to a fundamental idea or principle that underlies the structure and behavior of programming languages, such as variables, loops, and functions.\",\"description_2\":\"Programming concepts are the building blocks of software development, encompassing techniques and methodologies like object-oriented programming and recursion that guide how code is written and organized.\",\"description_3\":\"Examples of programming concepts include data types, control structures, and algorithms, which are essential for creating efficient and effective software solutions.\"}', refusal=None, role='assistant', function_call=None, tool_calls=[], parsed=Response(description_1='A programming concept refers to a fundamental idea or principle that underlies the structure and behavior of programming languages, such as variables, loops, and functions.', description_2='Programming concepts are the building blocks of software development, encompassing techniques and methodologies like object-oriented programming and recursion that guide how code is written and organized.', description_3='Examples of programming concepts include data types, control structures, and algorithms, which are essential for creating efficient and effective software solutions.')))], created=1727815397, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f85bea6784', usage=CompletionUsage(completion_tokens=98, prompt_tokens=2080, total_tokens=2178, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
