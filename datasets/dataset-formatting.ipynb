{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import (\n",
    "    ClassLabel,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(words: List[str], ner_tags: List[int]) -> str:\n",
    "\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        full_label = tag\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "    return line1 + \"\\n\" + line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_data(file_path: Path) -> Dict[int, Dict[str, List[str]]]:\n",
    "    dataset = []\n",
    "    sentence_id = 0\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates a sentence boundary\n",
    "                if current_words:  # If we have collected words and tags for a sentence\n",
    "                    dataset.append(\n",
    "                        {\n",
    "                            \"id\": str(sentence_id),\n",
    "                            \"words\": current_words,\n",
    "                            \"ner_tags\": current_tags,\n",
    "                        }\n",
    "                    )\n",
    "                    sentence_id += 1\n",
    "                    current_words = []\n",
    "                    current_tags = []\n",
    "            else:\n",
    "                word, tag = line.split(\"\\t\")  # Split by tab\n",
    "                current_words.append(word)\n",
    "                current_tags.append(tag)\n",
    "\n",
    "        # Append the last sentence if the file doesn't end with a newline\n",
    "        if current_words:\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"id\": str(sentence_id),\n",
    "                    \"words\": current_words,\n",
    "                    \"ner_tags\": current_tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"~/Development/entity-recognition-datasets\").expanduser()\n",
    "assert repo_path.exists(), \"Please clone the repository with the datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = repo_path / \"data/GUM/CONLL-format/data/train/gum-train.conll\"\n",
    "assert file_path_train.exists(), f\"File not found at {file_path_train}\"\n",
    "file_path_test = repo_path / \"data/GUM/CONLL-format/data/test/gum-test.conll\"\n",
    "assert file_path_test.exists(), f\"File not found at {file_path_test}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the train set: 2495\n",
      "Number of sentences in the test set: 1000\n"
     ]
    }
   ],
   "source": [
    "train = load_conll_data(file_path_train)\n",
    "print(f\"Number of sentences in the train set: {len(train)}\")\n",
    "test = load_conll_data(file_path_test)\n",
    "print(f\"Number of sentences in the test set: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = random.sample(range(len(test)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 437\n",
      "\" Pregnant   women      in         Australia  are        getting    about      half       as         much       as         what       they       require    on         a          daily      basis      .          \n",
      "O B-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 189\n",
      "The     accident occurred at a          time       when the     mosque  was relatively uncrowded . \n",
      "B-event I-event  O        O  B-abstract I-abstract O    B-place I-place O   O          O         O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 411\n",
      "We             took quite a              few            new            girls          over there   back then in 2005   , leading into the     World   Cup     in the     Netherlands . \n",
      "B-organization O    O     B-organization I-organization I-organization I-organization O    B-event O    O    O  B-time O O       O    B-event I-event I-event O  B-place I-place     O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 432\n",
      "The      database was unveiled in 1999   as the      National Enforcement Management Information System   and      Intelligence System   . \n",
      "B-object I-object O   O        O  B-time O  B-object I-object I-object    I-object   I-object    I-object I-object I-object     I-object O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 792\n",
      "They     think that modernity  may endanger their      tradition  . \n",
      "B-person O     O    B-abstract O   O        B-abstract I-abstract O \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for id in random_ids:\n",
    "    print(f\"Sentence ID: {id}\")\n",
    "    print(check_labels(test[id][\"words\"], test[id][\"ner_tags\"]))\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = output_dir / \"gum\"\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(train)\n",
    "train_path = output_path / \"gum-train.jsonl\"\n",
    "df_train.to_json(\n",
    "    path_or_buf=train_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "df_test = pd.DataFrame(test)\n",
    "test_path = output_path / \"gum-test.jsonl\"\n",
    "df_test.to_json(\n",
    "    path_or_buf=test_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sentences in the train set: {len(df_train)}\")\n",
    "print(f\"Label count in train set: {df_train['ner_tags'].explode().value_counts()}\")\n",
    "print(f\"Number of sentences in the test set: {len(df_test)}\")\n",
    "print(f\"Label count in test set: {df_test['ner_tags'].explode().value_counts(normalize=)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df_train[\"ner_tags\"].explode().value_counts().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_labels(labels):\n",
    "    # Separate the 'O' label from the rest\n",
    "    o_labels = [label for label in labels if label == \"O\"]\n",
    "\n",
    "    # Separate B- labels and corresponding I- labels\n",
    "    b_labels = sorted([label for label in labels if label.startswith(\"B-\")])\n",
    "    i_labels = [label for label in labels if label.startswith(\"I-\")]\n",
    "\n",
    "    # Sort I- labels based on their corresponding B- labels\n",
    "    sorted_labels = o_labels  # 'O' first\n",
    "    for b_label in b_labels:\n",
    "        sorted_labels.append(b_label)\n",
    "        # Add the corresponding I- label\n",
    "        corresponding_i_labels = [\n",
    "            i_label for i_label in i_labels if i_label[2:] == b_label[2:]\n",
    "        ]\n",
    "        sorted_labels.extend(corresponding_i_labels)\n",
    "\n",
    "    return sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names_sorted = sort_labels(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features(\n",
    "    {\n",
    "        \"id\": Value(dtype=\"string\"),\n",
    "        \"words\": Sequence(feature=Value(dtype=\"string\")),\n",
    "        \"ner_tags\": Sequence(feature=ClassLabel(names=label_names_sorted)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_dataset_train = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": str(train_path), \"test\": str(test_path)},\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_dataset_train.push_to_hub(repo_id=\"Studeni/GUM-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pust_to_hf(\n",
    "    repo_id: str,\n",
    "    label_names: List[str],\n",
    "    data_files: Dict[str, str] | str,\n",
    "    test_split_percentage: float = 0.2,\n",
    ") -> DatasetDict:\n",
    "    features = Features(\n",
    "        {\n",
    "            \"id\": Value(dtype=\"string\"),\n",
    "            \"words\": Sequence(feature=Value(dtype=\"string\")),\n",
    "            \"ner_tags\": Sequence(feature=ClassLabel(names=label_names)),\n",
    "        }\n",
    "    )\n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    if isinstance(data_files, str):\n",
    "        dataset = dataset[\"train\"].train_test_split(\n",
    "            test_size=test_split_percentage, seed=42\n",
    "        )\n",
    "\n",
    "    dataset.push_to_hub(repo_id=repo_id)\n",
    "    print(f\"Dataset {repo_id} pushed to the hub\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def sort_labels(labels):\n",
    "    # Separate the 'O' label from the rest\n",
    "    o_labels = [label for label in labels if label == \"O\"]\n",
    "\n",
    "    # Separate B- labels and corresponding I- labels\n",
    "    b_labels = sorted([label for label in labels if label.startswith(\"B-\")])\n",
    "    i_labels = [label for label in labels if label.startswith(\"I-\")]\n",
    "\n",
    "    # Sort I- labels based on their corresponding B- labels\n",
    "    sorted_labels = o_labels  # 'O' first\n",
    "    for b_label in b_labels:\n",
    "        sorted_labels.append(b_label)\n",
    "        # Add the corresponding I- label\n",
    "        corresponding_i_labels = [\n",
    "            i_label for i_label in i_labels if i_label[2:] == b_label[2:]\n",
    "        ]\n",
    "        sorted_labels.extend(corresponding_i_labels)\n",
    "\n",
    "    return sorted_labels\n",
    "\n",
    "\n",
    "def print_analysis(df: pd.DataFrame):\n",
    "    print(f\"Number of sentences in the dataset: {len(df)}\")\n",
    "    print(f\"Label count in dataset: {df['ner_tags'].explode().value_counts()}\")\n",
    "\n",
    "\n",
    "def vizualize_ner_dataset(dataset: DatasetDict, num_samples: int = 5):\n",
    "    random_ids = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "    print(\"=\" * 50 + \"START\" + \"=\" * 50)\n",
    "    for id in random_ids:\n",
    "        print(f\"Sentence ID: {id}\")\n",
    "        print(check_labels(dataset[id][\"words\"], dataset[id][\"ner_tags\"]))\n",
    "        print(\"-\" * 100)\n",
    "    print(\"=\" * 50 + \"END\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "def proces_ner_dataset(file_path: Path, output_path: Path = None) -> pd.DataFrame:\n",
    "    assert file_path.exists(), f\"File not found at {file_path_train}\"\n",
    "    if output_path is None:\n",
    "        assert output_path.suffix == \".jsonl\", \"Output path should be a JSONL file\"\n",
    "\n",
    "    output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    raw_data = load_conll_data(file_path_train)\n",
    "    print(f\"Number of sentences in the train set: {len(raw_data)}\")\n",
    "    vizualize_ner_dataset(raw_data)\n",
    "\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    if output_path is not None:\n",
    "        df.to_json(\n",
    "            path_or_buf=output_path,\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the train set: 2495\n",
      "==================================================START==================================================\n",
      "Sentence ID: 1565\n",
      "15       players  \n",
      "B-person I-person \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 387\n",
      "Belle    \n",
      "B-person \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 1865\n",
      "Read up on different  types      of         basil      and pick out one        - or several    - that appeal to you      , then order the     seeds   or buy them    at a       garden  store   . \n",
      "O    O  O  B-abstract I-abstract I-abstract I-abstract O   O    O   B-abstract O O  B-abstract O O    O      O  B-person O O    O     B-plant I-plant O  O   B-plant O  B-place I-place I-place O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 662\n",
      "When Tulsa   expanded beyond the     bounds  of      its     original plat    , the     expanded areas   were platted in alignment with the        points     of         the        compass    . \n",
      "O    B-place O        O      B-place I-place I-place I-place I-place  I-place O B-place I-place  I-place O    O       O  O         O    B-abstract I-abstract I-abstract I-abstract I-abstract O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 497\n",
      "They    say adding     the        mineral    to         salt       is         the        simplest   and        most       effective  method     of         preventing iodine     deficiency disorders  . \n",
      "B-event O   B-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "==================================================END==================================================\n"
     ]
    }
   ],
   "source": [
    "file_path = repo_path / \"data/wikigold/CONLL-format/data/wikigold.conll.txt\"\n",
    "dataset_name = file_path.parts[6]\n",
    "output_path = output_dir / dataset_name / \"wikigold.jsonl\"\n",
    "\n",
    "df = proces_ner_dataset(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the dataset: 2495\n",
      "Label count in dataset: ner_tags\n",
      "O                 20460\n",
      "I-abstract         4687\n",
      "I-event            2707\n",
      "I-place            2212\n",
      "B-abstract         2002\n",
      "B-person           1920\n",
      "I-person           1866\n",
      "I-object           1732\n",
      "B-place            1150\n",
      "B-object           1017\n",
      "B-event             738\n",
      "I-time              663\n",
      "I-organization      552\n",
      "I-substance         458\n",
      "B-time              401\n",
      "B-organization      397\n",
      "B-substance         278\n",
      "I-quantity          203\n",
      "I-plant             166\n",
      "B-plant             144\n",
      "B-animal            141\n",
      "I-animal            120\n",
      "B-quantity           97\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbf10326813403ab7a9fe510f8b03ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e475abc704ce457c923efa1145321aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec43785d59b476f992bb43f5e8e2c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f0146a9cc24ead9166e616a94991fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Studeni/Wikigold-NER pushed to the hub\n"
     ]
    }
   ],
   "source": [
    "label_names = list(df_train[\"ner_tags\"].explode().value_counts().keys())\n",
    "label_names = sort_labels(label_names)\n",
    "\n",
    "dataset = pust_to_hf(\n",
    "    repo_id=\"Studeni/Wikigold-NER\",\n",
    "    label_names=label_names,\n",
    "    data_files=str(output_path),\n",
    "    test_split_percentage=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the dataset: 1996\n",
      "Label count in dataset: ner_tags\n",
      "0     16334\n",
      "2      3790\n",
      "6      2158\n",
      "14     1740\n",
      "1      1621\n",
      "11     1515\n",
      "12     1502\n",
      "8      1381\n",
      "13      920\n",
      "7       809\n",
      "5       581\n",
      "22      542\n",
      "10      460\n",
      "20      385\n",
      "9       333\n",
      "21      320\n",
      "19      237\n",
      "18      168\n",
      "16      125\n",
      "15      116\n",
      "3       115\n",
      "4       106\n",
      "17       78\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_analysis(dataset[\"train\"].to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the dataset: 499\n",
      "Label count in dataset: ner_tags\n",
      "0     4126\n",
      "2      897\n",
      "6      549\n",
      "14     472\n",
      "11     405\n",
      "1      381\n",
      "12     364\n",
      "8      351\n",
      "13     230\n",
      "7      208\n",
      "5      157\n",
      "22     121\n",
      "10      92\n",
      "21      81\n",
      "20      73\n",
      "9       64\n",
      "16      41\n",
      "19      41\n",
      "18      35\n",
      "15      28\n",
      "3       26\n",
      "17      19\n",
      "4       14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_analysis(dataset[\"test\"].to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
