{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import (\n",
    "    ClassLabel,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(words: List[str], ner_tags: List[int]) -> str:\n",
    "\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        full_label = tag\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "    return line1 + \"\\n\" + line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_data(file_path: Path) -> Dict[int, Dict[str, List[str]]]:\n",
    "    dataset = []\n",
    "    sentence_id = 0\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates a sentence boundary\n",
    "                if current_words:  # If we have collected words and tags for a sentence\n",
    "                    dataset.append(\n",
    "                        {\n",
    "                            \"id\": str(sentence_id),\n",
    "                            \"words\": current_words,\n",
    "                            \"ner_tags\": current_tags,\n",
    "                        }\n",
    "                    )\n",
    "                    sentence_id += 1\n",
    "                    current_words = []\n",
    "                    current_tags = []\n",
    "            else:\n",
    "                word, tag = line.split(\"\\t\")  # Split by tab\n",
    "                current_words.append(word)\n",
    "                current_tags.append(tag)\n",
    "\n",
    "        # Append the last sentence if the file doesn't end with a newline\n",
    "        if current_words:\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"id\": str(sentence_id),\n",
    "                    \"words\": current_words,\n",
    "                    \"ner_tags\": current_tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"~/Development/entity-recognition-datasets\").expanduser()\n",
    "assert repo_path.exists(), \"Please clone the repository with the datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = repo_path / \"data/GUM/CONLL-format/data/train/gum-train.conll\"\n",
    "assert file_path_train.exists(), f\"File not found at {file_path_train}\"\n",
    "file_path_test = repo_path / \"data/GUM/CONLL-format/data/test/gum-test.conll\"\n",
    "assert file_path_test.exists(), f\"File not found at {file_path_test}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the train set: 2495\n",
      "Number of sentences in the test set: 1000\n"
     ]
    }
   ],
   "source": [
    "train = load_conll_data(file_path_train)\n",
    "print(f\"Number of sentences in the train set: {len(train)}\")\n",
    "test = load_conll_data(file_path_test)\n",
    "print(f\"Number of sentences in the test set: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = random.sample(range(len(test)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 437\n",
      "\" Pregnant   women      in         Australia  are        getting    about      half       as         much       as         what       they       require    on         a          daily      basis      .          \n",
      "O B-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 189\n",
      "The     accident occurred at a          time       when the     mosque  was relatively uncrowded . \n",
      "B-event I-event  O        O  B-abstract I-abstract O    B-place I-place O   O          O         O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 411\n",
      "We             took quite a              few            new            girls          over there   back then in 2005   , leading into the     World   Cup     in the     Netherlands . \n",
      "B-organization O    O     B-organization I-organization I-organization I-organization O    B-event O    O    O  B-time O O       O    B-event I-event I-event O  B-place I-place     O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 432\n",
      "The      database was unveiled in 1999   as the      National Enforcement Management Information System   and      Intelligence System   . \n",
      "B-object I-object O   O        O  B-time O  B-object I-object I-object    I-object   I-object    I-object I-object I-object     I-object O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 792\n",
      "They     think that modernity  may endanger their      tradition  . \n",
      "B-person O     O    B-abstract O   O        B-abstract I-abstract O \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for id in random_ids:\n",
    "    print(f\"Sentence ID: {id}\")\n",
    "    print(check_labels(test[id][\"words\"], test[id][\"ner_tags\"]))\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = output_dir / \"gum\"\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(train)\n",
    "train_path = output_path / \"gum-train.jsonl\"\n",
    "df_train.to_json(\n",
    "    path_or_buf=train_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "df_test = pd.DataFrame(test)\n",
    "test_path = output_path / \"gum-test.jsonl\"\n",
    "df_test.to_json(\n",
    "    path_or_buf=test_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df_train[\"ner_tags\"].explode().value_counts().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_labels(labels):\n",
    "    # Separate the 'O' label from the rest\n",
    "    o_labels = [label for label in labels if label == \"O\"]\n",
    "\n",
    "    # Separate B- labels and corresponding I- labels\n",
    "    b_labels = sorted([label for label in labels if label.startswith(\"B-\")])\n",
    "    i_labels = [label for label in labels if label.startswith(\"I-\")]\n",
    "\n",
    "    # Sort I- labels based on their corresponding B- labels\n",
    "    sorted_labels = o_labels  # 'O' first\n",
    "    for b_label in b_labels:\n",
    "        sorted_labels.append(b_label)\n",
    "        # Add the corresponding I- label\n",
    "        corresponding_i_labels = [\n",
    "            i_label for i_label in i_labels if i_label[2:] == b_label[2:]\n",
    "        ]\n",
    "        sorted_labels.extend(corresponding_i_labels)\n",
    "\n",
    "    return sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names_sorted = sort_labels(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features(\n",
    "    {\n",
    "        \"id\": Value(dtype=\"string\"),\n",
    "        \"words\": Sequence(feature=Value(dtype=\"string\")),\n",
    "        \"ner_tags\": Sequence(feature=ClassLabel(names=label_names_sorted)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_dataset_train = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": str(train_path), \"test\": str(test_path)},\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_dataset_train.push_to_hub(repo_id=\"Studeni/GUM-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
