{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import (\n",
    "    ClassLabel,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(words: List[str], ner_tags: List[int]) -> str:\n",
    "\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "\n",
    "    for word, tag in zip(words, ner_tags):\n",
    "        full_label = tag\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "    return line1 + \"\\n\" + line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_data(\n",
    "    file_path: Path, split_by: str = \"\\t\"\n",
    ") -> Dict[int, Dict[str, List[str]]]:\n",
    "    dataset = []\n",
    "    sentence_id = 0\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates a sentence boundary\n",
    "                if current_words:  # If we have collected words and tags for a sentence\n",
    "                    dataset.append(\n",
    "                        {\n",
    "                            \"id\": str(sentence_id),\n",
    "                            \"words\": current_words,\n",
    "                            \"ner_tags\": current_tags,\n",
    "                        }\n",
    "                    )\n",
    "                    sentence_id += 1\n",
    "                    current_words = []\n",
    "                    current_tags = []\n",
    "            else:\n",
    "                word, tag = line.split(split_by)  # Split by tab\n",
    "                current_words.append(word)\n",
    "                current_tags.append(tag)\n",
    "\n",
    "        # Append the last sentence if the file doesn't end with a newline\n",
    "        if current_words:\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"id\": str(sentence_id),\n",
    "                    \"words\": current_words,\n",
    "                    \"ner_tags\": current_tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"~/Development/entity-recognition-datasets\").expanduser()\n",
    "assert repo_path.exists(), \"Please clone the repository with the datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = repo_path / \"data/GUM/CONLL-format/data/train/gum-train.conll\"\n",
    "assert file_path_train.exists(), f\"File not found at {file_path_train}\"\n",
    "file_path_test = repo_path / \"data/GUM/CONLL-format/data/test/gum-test.conll\"\n",
    "assert file_path_test.exists(), f\"File not found at {file_path_test}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the train set: 2495\n",
      "Number of sentences in the test set: 1000\n"
     ]
    }
   ],
   "source": [
    "train = load_conll_data(file_path_train)\n",
    "print(f\"Number of sentences in the train set: {len(train)}\")\n",
    "test = load_conll_data(file_path_test)\n",
    "print(f\"Number of sentences in the test set: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = random.sample(range(len(test)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 437\n",
      "\" Pregnant   women      in         Australia  are        getting    about      half       as         much       as         what       they       require    on         a          daily      basis      .          \n",
      "O B-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract I-abstract \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 189\n",
      "The     accident occurred at a          time       when the     mosque  was relatively uncrowded . \n",
      "B-event I-event  O        O  B-abstract I-abstract O    B-place I-place O   O          O         O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 411\n",
      "We             took quite a              few            new            girls          over there   back then in 2005   , leading into the     World   Cup     in the     Netherlands . \n",
      "B-organization O    O     B-organization I-organization I-organization I-organization O    B-event O    O    O  B-time O O       O    B-event I-event I-event O  B-place I-place     O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 432\n",
      "The      database was unveiled in 1999   as the      National Enforcement Management Information System   and      Intelligence System   . \n",
      "B-object I-object O   O        O  B-time O  B-object I-object I-object    I-object   I-object    I-object I-object I-object     I-object O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 792\n",
      "They     think that modernity  may endanger their      tradition  . \n",
      "B-person O     O    B-abstract O   O        B-abstract I-abstract O \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for id in random_ids:\n",
    "    print(f\"Sentence ID: {id}\")\n",
    "    print(check_labels(test[id][\"words\"], test[id][\"ner_tags\"]))\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = output_dir / \"gum\"\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(train)\n",
    "train_path = output_path / \"gum-train.jsonl\"\n",
    "df_train.to_json(\n",
    "    path_or_buf=train_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "df_test = pd.DataFrame(test)\n",
    "test_path = output_path / \"gum-test.jsonl\"\n",
    "df_test.to_json(\n",
    "    path_or_buf=test_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sentences in the train set: {len(df_train)}\")\n",
    "print(f\"Label count in train set: {df_train['ner_tags'].explode().value_counts()}\")\n",
    "print(f\"Number of sentences in the test set: {len(df_test)}\")\n",
    "print(f\"Label count in test set: {df_test['ner_tags'].explode().value_counts(normalize=)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df_train[\"ner_tags\"].explode().value_counts().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_labels(labels):\n",
    "    # Separate the 'O' label from the rest\n",
    "    o_labels = [label for label in labels if label == \"O\"]\n",
    "\n",
    "    # Separate B- labels and corresponding I- labels\n",
    "    b_labels = sorted([label for label in labels if label.startswith(\"B-\")])\n",
    "    i_labels = [label for label in labels if label.startswith(\"I-\")]\n",
    "\n",
    "    # Sort I- labels based on their corresponding B- labels\n",
    "    sorted_labels = o_labels  # 'O' first\n",
    "    for b_label in b_labels:\n",
    "        sorted_labels.append(b_label)\n",
    "        # Add the corresponding I- label\n",
    "        corresponding_i_labels = [\n",
    "            i_label for i_label in i_labels if i_label[2:] == b_label[2:]\n",
    "        ]\n",
    "        sorted_labels.extend(corresponding_i_labels)\n",
    "\n",
    "    return sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names_sorted = sort_labels(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features(\n",
    "    {\n",
    "        \"id\": Value(dtype=\"string\"),\n",
    "        \"words\": Sequence(feature=Value(dtype=\"string\")),\n",
    "        \"ner_tags\": Sequence(feature=ClassLabel(names=label_names_sorted)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_dataset_train = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": str(train_path), \"test\": str(test_path)},\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_dataset_train.push_to_hub(repo_id=\"Studeni/GUM-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(labels: List[str]) -> List[str]:\n",
    "    \"\"\"This method fixes the labels to be in the correct IOB format. Wikigold only has O and I tags, but we need to have B, I and O tags.\"\"\"\n",
    "    if any([\"-\" in label for label in labels if label != \"O\"]):\n",
    "        labels = [label.split(\"-\")[-1] for label in labels]\n",
    "\n",
    "    new_labels = []\n",
    "    current_label = None\n",
    "    for label in labels:\n",
    "        if current_label is None:\n",
    "            current_label = label\n",
    "            if label == \"O\":\n",
    "                new_labels.append(label)\n",
    "            else:\n",
    "                new_labels.append(f\"B-{label}\")\n",
    "        else:\n",
    "            if label == \"O\":\n",
    "                new_labels.append(label)\n",
    "                current_label = label\n",
    "            else:\n",
    "                if label == current_label:\n",
    "                    new_labels.append(f\"I-{current_label}\")\n",
    "                else:\n",
    "                    new_labels.append(f\"B-{label}\")\n",
    "                    current_label = label\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def pust_to_hf(\n",
    "    repo_id: str,\n",
    "    label_names: List[str],\n",
    "    data_files: Dict[str, str] | str,\n",
    "    test_split_percentage: float = 0.2,\n",
    ") -> DatasetDict:\n",
    "    features = Features(\n",
    "        {\n",
    "            \"id\": Value(dtype=\"string\"),\n",
    "            \"words\": Sequence(feature=Value(dtype=\"string\")),\n",
    "            \"ner_tags\": Sequence(feature=ClassLabel(names=label_names)),\n",
    "        }\n",
    "    )\n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    if isinstance(data_files, str):\n",
    "        dataset = dataset[\"train\"].train_test_split(\n",
    "            test_size=test_split_percentage, seed=42\n",
    "        )\n",
    "\n",
    "    dataset.push_to_hub(repo_id=repo_id)\n",
    "    print(f\"Dataset {repo_id} pushed to the hub\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def sort_labels(labels):\n",
    "    # Separate the 'O' label from the rest\n",
    "    o_labels = [label for label in labels if label == \"O\"]\n",
    "\n",
    "    # Separate B- labels and corresponding I- labels\n",
    "    b_labels = sorted([label for label in labels if label.startswith(\"B-\")])\n",
    "    i_labels = [label for label in labels if label.startswith(\"I-\")]\n",
    "\n",
    "    # Sort I- labels based on their corresponding B- labels\n",
    "    sorted_labels = o_labels  # 'O' first\n",
    "    for b_label in b_labels:\n",
    "        sorted_labels.append(b_label)\n",
    "        # Add the corresponding I- label\n",
    "        corresponding_i_labels = [\n",
    "            i_label for i_label in i_labels if i_label[2:] == b_label[2:]\n",
    "        ]\n",
    "        sorted_labels.extend(corresponding_i_labels)\n",
    "\n",
    "    return sorted_labels\n",
    "\n",
    "\n",
    "def print_analysis(df: pd.DataFrame):\n",
    "    print(f\"Number of sentences in the dataset: {len(df)}\")\n",
    "    print(f\"Label count in dataset: {df['ner_tags'].explode().value_counts()}\")\n",
    "\n",
    "\n",
    "def vizualize_ner_dataset(dataset: DatasetDict, num_samples: int = 5):\n",
    "    random_ids = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "    print(\"=\" * 50 + \"START\" + \"=\" * 50)\n",
    "    for id in random_ids:\n",
    "        print(f\"Sentence ID: {id}\")\n",
    "        print(check_labels(dataset[id][\"words\"], dataset[id][\"ner_tags\"]))\n",
    "        print(\"-\" * 100)\n",
    "    print(\"=\" * 50 + \"END\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "def proces_ner_dataset(\n",
    "    file_path: Path, output_path: Path = None, split_by: str = \"\\t\"\n",
    ") -> pd.DataFrame:\n",
    "    assert file_path.exists(), f\"File not found at {file_path}\"\n",
    "    if output_path is None:\n",
    "        assert output_path.suffix == \".jsonl\", \"Output path should be a JSONL file\"\n",
    "\n",
    "    output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    raw_data = load_conll_data(file_path, split_by=split_by)\n",
    "    print(f\"Number of sentences in the train set: {len(raw_data)}\")\n",
    "    vizualize_ner_dataset(raw_data)\n",
    "\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    if output_path is not None:\n",
    "        df.to_json(\n",
    "            path_or_buf=output_path,\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the train set: 1841\n",
      "==================================================START==================================================\n",
      "Sentence ID: 295\n",
      "They were compelled to wear a distinctive dress , to which , in some places , was attached the foot of a goose or duck ( whence they were sometimes called Canards ) . \n",
      "O    O    O         O  O    O O           O     O O  O     O O  O    O      O O   O        O   O    O  O O     O  O    O O      O    O    O         O      I-MISC  O O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 987\n",
      "In 1928 , Henrique and José  Pedr  left the company , and Caloi was directed by Guido Caloi alone . \n",
      "O  O    O I-PER    O   I-PER I-PER O    O   O       O O   I-ORG O   O        O  I-PER I-PER O     O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 1225\n",
      "The Clarence Dock  is a dock , on the River Mersey and part of the Port  of    Liverpool it is situated in the northern end of the dock system , connected to Salisbury Dock  to the north and Trafalgar Dock  to the south . \n",
      "O   I-LOC    I-LOC O  O O    O O  O   I-LOC I-LOC  O   O    O  O   I-LOC I-LOC I-LOC     O  O  O        O  O   O        O   O  O   O    O      O O         O  I-LOC     I-LOC O  O   O     O   I-LOC     I-LOC O  O   O     O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 927\n",
      "Box cars # 59-62 were the first 30-foot freight cars built for any 2-foot gauge railroad in Maine . \n",
      "O   O    O O     O    O   O     O       O       O    O     O   O   O      O     O        O  I-LOC O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence ID: 1816\n",
      "Fairfax Connector deployed SmarTrip readers on its buses in May 2007 , extending the use of this Washington Metro fare-payment system to Fairfax County . \n",
      "I-ORG   I-ORG     O        I-ORG    O       O  O   O     O  O   O    O O         O   O   O  O    I-LOC      I-LOC O            O      O  I-LOC   I-LOC  O \n",
      "----------------------------------------------------------------------------------------------------\n",
      "==================================================END==================================================\n"
     ]
    }
   ],
   "source": [
    "file_path = repo_path / \"data/wikigold/CONLL-format/data/wikigold.conll.txt\"\n",
    "dataset_name = file_path.parts[6]\n",
    "output_path = output_dir / dataset_name / \"wikigold.jsonl\"\n",
    "\n",
    "df = proces_ner_dataset(file_path, output_path, split_by=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ner_tags\"] = df[\"ner_tags\"].apply(format_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[010, is, the, tenth, album, from, Japanese, P...</td>\n",
       "      <td>[B-MISC, O, O, O, O, O, B-MISC, O, O, O, B-ORG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[This, album, proved, to, be, more, commercial...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-MISC, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Founding, member, Kojima, Minoru, played, gui...</td>\n",
       "      <td>[O, O, B-PER, I-PER, O, O, O, B-MISC, I-MISC, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[XXX, can, of, This, had, a, different, meanin...</td>\n",
       "      <td>[B-MISC, I-MISC, I-MISC, I-MISC, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[it, was, later, explained, that, the, song, w...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                              words  \\\n",
       "0  0  [010, is, the, tenth, album, from, Japanese, P...   \n",
       "1  1  [This, album, proved, to, be, more, commercial...   \n",
       "2  2  [Founding, member, Kojima, Minoru, played, gui...   \n",
       "3  3  [XXX, can, of, This, had, a, different, meanin...   \n",
       "4  4  [it, was, later, explained, that, the, song, w...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [B-MISC, O, O, O, O, O, B-MISC, O, O, O, B-ORG...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, B-MISC, O, O...  \n",
       "2  [O, O, B-PER, I-PER, O, O, O, B-MISC, I-MISC, ...  \n",
       "3  [B-MISC, I-MISC, I-MISC, I-MISC, O, O, O, O, O...  \n",
       "4  [O, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the dataset: 1841\n",
      "Label count in dataset: ner_tags\n",
      "O         32721\n",
      "I-ORG      1060\n",
      "B-LOC      1014\n",
      "B-PER       934\n",
      "B-ORG       898\n",
      "B-MISC      712\n",
      "I-PER       700\n",
      "I-MISC      680\n",
      "I-LOC       433\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5727a1222d4ef6b2ea0f38d9c13802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969f1195e624410985b8e888f772436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e6267654bb4419bd742a73693583d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f6767640074e97b06f209f982ab91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdddf1d26674c2e8457e8bf48b68a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Studeni/Wikigold-NER pushed to the hub\n"
     ]
    }
   ],
   "source": [
    "label_names = list(df[\"ner_tags\"].explode().value_counts().keys())\n",
    "label_names = sort_labels(label_names)\n",
    "\n",
    "dataset = pust_to_hf(\n",
    "    repo_id=\"Studeni/Wikigold-NER\",\n",
    "    label_names=label_names,\n",
    "    data_files=str(output_path),\n",
    "    test_split_percentage=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the dataset: 1472\n",
      "Label count in dataset: ner_tags\n",
      "O         26086\n",
      "I-ORG      1470\n",
      "I-PER      1303\n",
      "I-LOC      1175\n",
      "I-MISC     1151\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ner_tags\n",
       "O         83.649190\n",
       "I-ORG      4.713805\n",
       "I-PER      4.178291\n",
       "I-LOC      3.767837\n",
       "I-MISC     3.690877\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_train[\"ner_tags\"] = df_train[\"ner_tags\"].apply(\n",
    "    lambda x: [label_names[id] for id in x]\n",
    ")\n",
    "print_analysis(df_train)\n",
    "df_train[\"ner_tags\"].explode().value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the dataset: 369\n",
      "Label count in dataset: ner_tags\n",
      "O         6635\n",
      "I-ORG      488\n",
      "I-PER      331\n",
      "I-LOC      272\n",
      "I-MISC     241\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ner_tags\n",
       "O         83.281034\n",
       "I-ORG      6.125267\n",
       "I-PER      4.154638\n",
       "I-LOC      3.414083\n",
       "I-MISC     3.024978\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = dataset[\"test\"].to_pandas()\n",
    "df_test[\"ner_tags\"] = df_test[\"ner_tags\"].apply(lambda x: [label_names[id] for id in x])\n",
    "print_analysis(df_test)\n",
    "df_test[\"ner_tags\"].explode().value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
